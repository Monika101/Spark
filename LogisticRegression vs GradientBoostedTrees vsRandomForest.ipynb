{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Backorder Prediction for Retailer, Logistic Regression vs Gradient Boosted Trees vs Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data file contains the historical data for the 8 weeks prior to the week we are trying to predict. The data was taken as weekly snapshots at the start of each week. Columns are defined as follows:\n",
    "\n",
    "sku - Random ID for the product;\n",
    "national_inv - Current inventory level for the part;\n",
    "lead_time - Transit time for product (if available);\n",
    "in_transit_qty - Amount of product in transit from source;\n",
    "forecast_3_month - Forecast sales for the next 3 months;\n",
    "forecast_6_month - Forecast sales for the next 6 months;\n",
    "forecast_9_month - Forecast sales for the next 9 months;\n",
    "sales_1_month - Sales quantity for the prior 1 month time period;\n",
    "sales_3_month - Sales quantity for the prior 3 month time period;\n",
    "sales_6_month - Sales quantity for the prior 6 month time period;\n",
    "sales_9_month - Sales quantity for the prior 9 month time period;\n",
    "min_bank - Minimum recommend amount to stock;\n",
    "potential_issue - Source issue for part identified;\n",
    "pieces_past_due - Parts overdue from source;\n",
    "perf_6_month_avg - Source performance for prior 6 month period;\n",
    "perf_12_month_avg - Source performance for prior 12 month period;\n",
    "local_bo_qty - Amount of stock orders overdue;\n",
    "deck_risk - Part risk flag;\n",
    "oe_constraint - Part risk flag;\n",
    "ppap_risk - Part risk flag;\n",
    "stop_auto_buy - Part risk flag;\n",
    "rev_stop - Part risk flag;\n",
    "went_on_backorder - Product actually went on backorder;\n",
    "This is the target value. national_inv - Current inventory level for the part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Find Spark and set its location\n",
    "\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('backorder').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = spark.read.csv(\"Backorder_Dataset_small.csv\", inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sku: string (nullable = true)\n",
      " |-- national_inv: integer (nullable = true)\n",
      " |-- lead_time: integer (nullable = true)\n",
      " |-- in_transit_qty: integer (nullable = true)\n",
      " |-- forecast_3_month: integer (nullable = true)\n",
      " |-- forecast_6_month: integer (nullable = true)\n",
      " |-- forecast_9_month: integer (nullable = true)\n",
      " |-- sales_1_month: integer (nullable = true)\n",
      " |-- sales_3_month: integer (nullable = true)\n",
      " |-- sales_6_month: integer (nullable = true)\n",
      " |-- sales_9_month: integer (nullable = true)\n",
      " |-- min_bank: integer (nullable = true)\n",
      " |-- potential_issue: string (nullable = true)\n",
      " |-- pieces_past_due: integer (nullable = true)\n",
      " |-- perf_6_month_avg: double (nullable = true)\n",
      " |-- perf_12_month_avg: double (nullable = true)\n",
      " |-- local_bo_qty: integer (nullable = true)\n",
      " |-- deck_risk: string (nullable = true)\n",
      " |-- oe_constraint: string (nullable = true)\n",
      " |-- ppap_risk: string (nullable = true)\n",
      " |-- stop_auto_buy: string (nullable = true)\n",
      " |-- rev_stop: string (nullable = true)\n",
      " |-- went_on_backorder: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Set certain types of data as 'double'\n",
    "### StringIndexer is to create quantitative data field out of 'went_on_backorder'\n",
    "\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+\n",
      "|went_on_backorder| count|\n",
      "+-----------------+------+\n",
      "|             null|     1|\n",
      "|               No|239387|\n",
      "|              Yes|  2688|\n",
      "+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Show 'null' values\n",
    "\n",
    "train.groupBy('went_on_backorder').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+\n",
      "|went_on_backorder| count|\n",
      "+-----------------+------+\n",
      "|               No|239387|\n",
      "|              Yes|  2688|\n",
      "+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Drop the one record with 'null' value\n",
    "\n",
    "train = train.dropna(subset = 'went_on_backorder')\n",
    "train.groupBy('went_on_backorder').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Create 'backorder' column that transforms qualitative field into quantitative\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"went_on_backorder\", outputCol=\"backorder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Change data type for fields needed for the Logistic Regression model\n",
    "\n",
    "train = train.withColumn(\"national_inv\", train[\"national_inv\"].cast(DoubleType()))\n",
    "train = train.withColumn(\"lead_time\", train[\"lead_time\"].cast(DoubleType()))\n",
    "train = train.withColumn(\"in_transit_qty\", train[\"in_transit_qty\"].cast(DoubleType()))\n",
    "train = train.withColumn(\"min_bank\", train[\"min_bank\"].cast(DoubleType()))\n",
    "train = train.withColumn(\"local_bo_qty\", train[\"local_bo_qty\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|lead_time|count|\n",
      "+---------+-----+\n",
      "|      8.0|98533|\n",
      "|      0.0| 1515|\n",
      "|      7.0|   30|\n",
      "|     null|14724|\n",
      "|     35.0|    5|\n",
      "|     18.0|   32|\n",
      "|      1.0|    3|\n",
      "|     25.0|    1|\n",
      "|      4.0|18508|\n",
      "|     23.0|    2|\n",
      "|     11.0|  152|\n",
      "|     21.0|    7|\n",
      "|     14.0| 1506|\n",
      "|     22.0|   19|\n",
      "|      3.0| 2349|\n",
      "|     19.0|    4|\n",
      "|     28.0|   12|\n",
      "|      2.0|46917|\n",
      "|     17.0|  537|\n",
      "|     10.0| 2056|\n",
      "+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### 'Lead_time' has null values too.\n",
    "\n",
    "train.groupBy('lead_time').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Calculate mean (average) value for 'lead_time' to replace 'null' value.\n",
    "\n",
    "from pyspark.sql.functions import mean\n",
    "meanValue = train.select(mean(train['lead_time']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### The actual mean (average) value for filling in 'null' values\n",
    "meanValue.collect()[0][0]\n",
    "Value = meanValue.collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Use meanValue to fill in 'null' in \"lead_time\".\n",
    "train = train.na.fill(Value, subset=['lead_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|        lead_time|count|\n",
      "+-----------------+-----+\n",
      "|              8.0|98533|\n",
      "|              0.0| 1515|\n",
      "|              7.0|   30|\n",
      "|             35.0|    5|\n",
      "|             18.0|   32|\n",
      "|7.923017712699746|14724|\n",
      "|              1.0|    3|\n",
      "|             25.0|    1|\n",
      "|              4.0|18508|\n",
      "|             23.0|    2|\n",
      "|             11.0|  152|\n",
      "|             21.0|    7|\n",
      "|             14.0| 1506|\n",
      "|             22.0|   19|\n",
      "|              3.0| 2349|\n",
      "|             19.0|    4|\n",
      "|             28.0|   12|\n",
      "|              2.0|46917|\n",
      "|             17.0|  537|\n",
      "|             10.0| 2056|\n",
      "+-----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Now show what 'lead_time' looks like once 'null' has been set to average (mean) value.\n",
    "\n",
    "train = train.withColumn(\"lead_time\", train[\"lead_time\"].cast(DoubleType()))\n",
    "train.groupBy('lead_time').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Import Pearson Correlation to check relationship between features.\n",
    "\n",
    "from pyspark.sql.functions import corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|corr(lead_time, national_inv)|\n",
      "+-----------------------------+\n",
      "|         0.003360543456619082|\n",
      "+-----------------------------+\n",
      "\n",
      "+-------------------------------+\n",
      "|corr(in_transit_qty, lead_time)|\n",
      "+-------------------------------+\n",
      "|           -0.00807130850689...|\n",
      "+-------------------------------+\n",
      "\n",
      "+------------------------------+\n",
      "|corr(min_bank, in_transit_qty)|\n",
      "+------------------------------+\n",
      "|            0.7229275709491002|\n",
      "+------------------------------+\n",
      "\n",
      "+-------------------------+\n",
      "|corr(lead_time, min_bank)|\n",
      "+-------------------------+\n",
      "|     -0.00693979549451...|\n",
      "+-------------------------+\n",
      "\n",
      "+----------------------------+\n",
      "|corr(min_bank, local_bo_qty)|\n",
      "+----------------------------+\n",
      "|        0.048200196504585985|\n",
      "+----------------------------+\n",
      "\n",
      "+--------------------------------+\n",
      "|corr(min_bank, perf_6_month_avg)|\n",
      "+--------------------------------+\n",
      "|            -0.00345017634879...|\n",
      "+--------------------------------+\n",
      "\n",
      "+---------------------------------------+\n",
      "|corr(in_transit_qty, perf_12_month_avg)|\n",
      "+---------------------------------------+\n",
      "|                   0.004046812190188296|\n",
      "+---------------------------------------+\n",
      "\n",
      "+-----------------------------------+\n",
      "|corr(in_transit_qty, sales_9_month)|\n",
      "+-----------------------------------+\n",
      "|                 0.5265367370835552|\n",
      "+-----------------------------------+\n",
      "\n",
      "+-------------------------------------+\n",
      "|corr(sales_9_month, forecast_9_month)|\n",
      "+-------------------------------------+\n",
      "|                   0.8606098866748187|\n",
      "+-------------------------------------+\n",
      "\n",
      "+-------------------------------------+\n",
      "|corr(sales_6_month, forecast_6_month)|\n",
      "+-------------------------------------+\n",
      "|                   0.7955021547041715|\n",
      "+-------------------------------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|corr(sales_9_month, min_bank)|\n",
      "+-----------------------------+\n",
      "|            0.831548523236603|\n",
      "+-----------------------------+\n",
      "\n",
      "+-------------------------------+\n",
      "|corr(min_bank, pieces_past_due)|\n",
      "+-------------------------------+\n",
      "|            0.27963571914818225|\n",
      "+-------------------------------+\n",
      "\n",
      "+---------------------------------+\n",
      "|corr(sales_9_month, local_bo_qty)|\n",
      "+---------------------------------+\n",
      "|              0.04988254980380944|\n",
      "+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Feature relationships\n",
    "\n",
    "train.select(corr('lead_time','national_inv')).show()\n",
    "train.select(corr('in_transit_qty','lead_time')).show()\n",
    "train.select(corr('min_bank','in_transit_qty')).show()\n",
    "train.select(corr('lead_time','min_bank')).show()\n",
    "train.select(corr('min_bank','local_bo_qty')).show()\n",
    "train.select(corr('min_bank','perf_6_month_avg')).show()\n",
    "train.select(corr('in_transit_qty','perf_12_month_avg')).show()\n",
    "train.select(corr('in_transit_qty','sales_9_month')).show()\n",
    "train.select(corr('sales_9_month','forecast_9_month')).show()\n",
    "train.select(corr('sales_6_month','forecast_6_month')).show()\n",
    "train.select(corr('sales_9_month','min_bank')).show()\n",
    "train.select(corr('min_bank','pieces_past_due')).show()\n",
    "train.select(corr('sales_9_month','local_bo_qty')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Assemble features vector for Logistic Regression\n",
    "\n",
    "assembler = VectorAssembler(inputCols = [\"national_inv\", \"lead_time\", \"in_transit_qty\", \"min_bank\", \"local_bo_qty\",\n",
    "                                        \"perf_6_month_avg\", \"perf_12_month_avg\"],\n",
    "                           outputCol = \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Transform 'train' DataFrame with new feature vector\n",
    "\n",
    "output = assembler.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Introduce quantitative output of qualitative 'went_on_backorder'\n",
    "\n",
    "outputIndexed = indexer.fit(output).transform(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------+---------+\n",
      "|features                                        |backorder|\n",
      "+------------------------------------------------+---------+\n",
      "|[62.0,7.923017712699746,0.0,1.0,0.0,-99.0,-99.0]|0.0      |\n",
      "|[9.0,7.923017712699746,0.0,1.0,0.0,-99.0,-99.0] |0.0      |\n",
      "|[17.0,8.0,0.0,0.0,0.0,0.92,0.95]                |0.0      |\n",
      "|[9.0,2.0,0.0,0.0,0.0,0.78,0.75]                 |0.0      |\n",
      "|[2.0,8.0,0.0,0.0,0.0,0.54,0.71]                 |0.0      |\n",
      "|[15.0,2.0,0.0,0.0,0.0,0.37,0.68]                |0.0      |\n",
      "|(7,[1,5,6],[7.923017712699746,-99.0,-99.0])     |0.0      |\n",
      "|[28.0,7.923017712699746,0.0,0.0,0.0,-99.0,-99.0]|0.0      |\n",
      "|[2.0,7.923017712699746,0.0,0.0,0.0,-99.0,-99.0] |0.0      |\n",
      "|[2.0,7.923017712699746,0.0,0.0,0.0,-99.0,-99.0] |0.0      |\n",
      "|[20.0,7.923017712699746,0.0,1.0,0.0,-99.0,-99.0]|0.0      |\n",
      "|(7,[1,5,6],[7.923017712699746,-99.0,-99.0])     |0.0      |\n",
      "|[13.0,7.923017712699746,0.0,0.0,0.0,-99.0,-99.0]|0.0      |\n",
      "|[208.0,16.0,0.0,0.0,0.0,0.66,0.64]              |0.0      |\n",
      "|(7,[1,5,6],[7.923017712699746,-99.0,-99.0])     |0.0      |\n",
      "|(7,[1,5,6],[2.0,0.37,0.34])                     |0.0      |\n",
      "|(7,[1,5,6],[7.923017712699746,-99.0,-99.0])     |0.0      |\n",
      "|[26.0,7.923017712699746,0.0,0.0,0.0,-99.0,-99.0]|0.0      |\n",
      "|[36.0,7.923017712699746,0.0,0.0,0.0,-99.0,-99.0]|0.0      |\n",
      "|[23.0,7.923017712699746,0.0,0.0,0.0,-99.0,-99.0]|0.0      |\n",
      "+------------------------------------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Show features with quantitative transformation completed\n",
    "\n",
    "outputIndexed.select(\"features\", \"backorder\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 'Data' will be split between training and test.\n",
    "\n",
    "data = outputIndexed.select(\"features\", \"backorder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Split training vs test data\n",
    "\n",
    "train_data, test_data = data.randomSplit([0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Logistic Regression Model\n",
    "\n",
    "lr = LogisticRegression(labelCol = \"backorder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrModel = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingModel = lrModel.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------------------+----------+\n",
      "| features|backorder|       rawPrediction|         probability|prediction|\n",
      "+---------+---------+--------------------+--------------------+----------+\n",
      "|(7,[],[])|      0.0|[3.91004493495093...|[0.98035409551558...|       0.0|\n",
      "|(7,[],[])|      0.0|[3.91004493495093...|[0.98035409551558...|       0.0|\n",
      "|(7,[],[])|      0.0|[3.91004493495093...|[0.98035409551558...|       0.0|\n",
      "+---------+---------+--------------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingModel.predictions.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Evaluate Logistic Regression model based on test_data that is fresh.\n",
    "\n",
    "predictionAndLables = lrModel.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+--------------------+--------------------+----------+\n",
      "|     features|backorder|       rawPrediction|         probability|prediction|\n",
      "+-------------+---------+--------------------+--------------------+----------+\n",
      "|    (7,[],[])|      0.0|[3.91004493495093...|[0.98035409551558...|       0.0|\n",
      "|(7,[0],[1.0])|      0.0|[3.91281781497300...|[0.98040742995279...|       0.0|\n",
      "|(7,[0],[5.0])|      0.0|[3.92390933506128...|[0.98061935225882...|       0.0|\n",
      "+-------------+---------+--------------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionAndLables.predictions.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Binary classification evaluation as options were '0.0' or '1.0'\n",
    "\n",
    "churn_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction',\n",
    "                                           labelCol='backorder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49997911096256686"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Result for classification, the same as random guess (~50%)\n",
    "\n",
    "auc = churn_eval.evaluate(predictionAndLables.predictions)\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Use Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Set up GBT Classified model\n",
    "\n",
    "gbt = GBTClassifier(labelCol=\"backorder\", featuresCol=\"features\", maxIter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### FIT 'train_data', meaning it has been transformed into including quantitative data ('backorder') and there are no\n",
    "### 'null' values. Data has also been split between 'training' and 'testing'.\n",
    "\n",
    "model = gbt.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TRANSFORM\n",
    "\n",
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+\n",
      "|prediction|backorder|     features|\n",
      "+----------+---------+-------------+\n",
      "|       0.0|      0.0|    (7,[],[])|\n",
      "|       0.0|      0.0|(7,[0],[1.0])|\n",
      "+----------+---------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"prediction\", \"backorder\", \"features\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.0117258\n"
     ]
    }
   ],
   "source": [
    "### Select (prediction, true label) and compute test error\n",
    "### Evaluate model on accuracy\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"backorder\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9882741535920727"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Roughly ~98 - 99%, much better than what random guess would be able to achieve (which is around 50%)\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(labelCol='backorder',featuresCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfcModel = rfc.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfcPredictions = rfcModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accEvaluator = MulticlassClassificationEvaluator(labelCol=\"backorder\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfcAcc = accEvaluator.evaluate(rfcPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A random forest ensemble had an accuracy of: 98.68%\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('A random forest ensemble had an accuracy of: {0:2.2f}%'.format(rfcAcc*100))\n",
    "print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### F1 metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accEvaluatorF1 = MulticlassClassificationEvaluator(labelCol=\"backorder\", predictionCol=\"prediction\", metricName=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfcAccF1 = accEvaluatorF1.evaluate(rfcPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A random forest ensemble had an accuracy of: 98.03%\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('A random forest ensemble had an accuracy of: {0:2.2f}%'.format(rfcAccF1*100))\n",
    "print('-'*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosted Trees have highest score at 98.8% accurancy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
